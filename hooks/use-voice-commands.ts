import createContextHook from '@nkzw/create-context-hook';
import AsyncStorage from '@react-native-async-storage/async-storage';
import * as Haptics from 'expo-haptics';

import { useCallback, useEffect, useMemo, useRef, useState } from 'react';
import { Platform, Alert, PermissionsAndroid } from 'react-native';
import { useI18n } from './use-i18n';

export interface VoiceCommand {
  id: string;
  key: string;
  customTrigger?: string;
  action: () => void;
}

interface RecordingState {
  isRecording: boolean;
  isProcessing: boolean;
  lastCommand?: string;
  error?: string;
  userHint?: string;
  isListening: boolean;
  isPersistentMode: boolean;
  isAuthorized: boolean;
  authorizationStatus: 'notDetermined' | 'denied' | 'restricted' | 'authorized';
  recognitionAvailable: boolean;
  confidenceThreshold: number;
  lastConfidence?: number;
}

export const [VoiceCommandProvider, useVoiceCommands] = createContextHook(() => {
  const { t, language } = useI18n();
  const [state, setState] = useState<RecordingState>({
    isRecording: false,
    isProcessing: false,
    isListening: false,
    isPersistentMode: false,
    isAuthorized: false,
    authorizationStatus: 'notDetermined',
    recognitionAvailable: false,
    userHint: undefined,
    confidenceThreshold: 0.7,
    lastConfidence: undefined,
  });
  const [customCommands, setCustomCommands] = useState<Record<string, string>>({});
  const recognitionRef = useRef<any>(null);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const persistentRecognitionRef = useRef<any>(null);
  const restartTimeoutRef = useRef<ReturnType<typeof setTimeout> | null>(null);
  const maxSessionTimerRef = useRef<ReturnType<typeof setTimeout> | null>(null);
  const speechRecognizerRef = useRef<any>(null);
  const audioEngineRef = useRef<any>(null);
  const recognitionTaskRef = useRef<any>(null);
  const recognitionRequestRef = useRef<any>(null);
  const webRetryCountRef = useRef<number>(0);
  const webMaxRetriesRef = useRef<number>(2);
  const webRestartingRef = useRef<boolean>(false);

  const loadCustomCommands = useCallback(async () => {
    try {
      const stored = await AsyncStorage.getItem('customCommands');
      if (stored) {
        setCustomCommands(JSON.parse(stored));
      }
      const thresholdRaw = await AsyncStorage.getItem('voiceConfidenceThreshold');
      if (thresholdRaw) {
        const value = Number(thresholdRaw);
        if (!Number.isNaN(value) && value >= 0 && value <= 1) {
          setState(prev => ({ ...prev, confidenceThreshold: value }));
        }
      }
    } catch (error) {
      console.error('Error loading custom commands or threshold:', error);
    }
  }, []);

  useEffect(() => {
    loadCustomCommands();
  }, [loadCustomCommands]);

  const saveCustomCommand = useCallback(async (commandId: string, trigger: string) => {
    const updated = { ...customCommands, [commandId]: trigger };
    setCustomCommands(updated);
    await AsyncStorage.setItem('customCommands', JSON.stringify(updated));
  }, [customCommands]);

  const setConfidenceThreshold = useCallback(async (value: number) => {
    const safe = Math.max(0, Math.min(1, value));
    setState(prev => ({ ...prev, confidenceThreshold: safe }));
    try {
      await AsyncStorage.setItem('voiceConfidenceThreshold', String(safe));
    } catch (e) {
      console.error('Failed to persist threshold', e);
    }
  }, []);

  // éšŽæ®µ 1ï¼šæŽˆæ¬Šèˆ‡ç’°å¢ƒè¨­å®š - ç´” Speech Framework å¯¦ç¾
  const requestSpeechAuthorization = useCallback(async () => {
    console.log('ðŸŽ¤ Requesting Speech Framework authorization...');
    
    if (Platform.OS === 'ios') {
      try {
        // ä½¿ç”¨ Speech Framework é€²è¡ŒèªžéŸ³è¾¨è­˜æŽˆæ¬Š
        // æ³¨æ„ï¼šé€™è£¡æˆ‘å€‘æ¨¡æ“¬ Speech Framework çš„è¡Œç‚º
        // å¯¦éš›ä¸Šåœ¨ React Native ä¸­éœ€è¦åŽŸç”Ÿæ¨¡çµ„æ”¯æ´
        console.log('iOS Speech Framework authorization request');
        
        // æª¢æŸ¥èªžéŸ³è¾¨è­˜æ˜¯å¦å¯ç”¨
        const isAvailable = true; // Speech Framework åœ¨ iOS 10+ éƒ½å¯ç”¨
        
        if (!isAvailable) {
          setState(prev => ({
            ...prev,
            error: 'æ‚¨çš„è£ç½®ä¸æ”¯æ´èªžéŸ³è¾¨è­˜åŠŸèƒ½',
            recognitionAvailable: false,
            authorizationStatus: 'restricted'
          }));
          return false;
        }
        
        // è«‹æ±‚èªžéŸ³è¾¨è­˜æŽˆæ¬Šï¼ˆSpeech Frameworkï¼‰
        // åœ¨å¯¦éš›å¯¦ç¾ä¸­ï¼Œé€™æœƒèª¿ç”¨ SFSpeechRecognizer.requestAuthorization()
        console.log('Requesting Speech Framework authorization...');
        
        // è«‹æ±‚éº¥å…‹é¢¨æ¬Šé™
        console.log('Requesting microphone permission...');
        
        // æ¨¡æ“¬æŽˆæ¬ŠæˆåŠŸï¼ˆåœ¨å¯¦éš›å¯¦ç¾ä¸­æœƒæœ‰çœŸå¯¦çš„æŽˆæ¬Šæµç¨‹ï¼‰
        const authStatus = 'authorized';
        const micPermission = true;
        
        const isAuthorized = authStatus === 'authorized' && micPermission;
        
        setState(prev => ({
          ...prev,
          isAuthorized,
          authorizationStatus: authStatus,
          recognitionAvailable: isAvailable,
          error: isAuthorized ? undefined : 'èªžéŸ³è¾¨è­˜æ¬Šé™è¢«æ‹’çµ•'
        }));
        
        console.log('âœ… Speech Framework authorization completed:', { authStatus, micPermission });
        return isAuthorized;
      } catch (error) {
        console.error('Speech Framework authorization failed:', error);
        // é™ç´šåˆ° Web Speech API
        return await requestWebSpeechAuthorization();
      }
    } else if (Platform.OS === 'android') {
      try {
        // Android ä½¿ç”¨æ¨™æº–éº¥å…‹é¢¨æ¬Šé™
        const granted = await PermissionsAndroid.request(
          PermissionsAndroid.PERMISSIONS.RECORD_AUDIO,
          {
            title: 'éº¥å…‹é¢¨æ¬Šé™',
            message: 'æ­¤æ‡‰ç”¨ç¨‹å¼éœ€è¦éº¥å…‹é¢¨æ¬Šé™ä¾†é€²è¡ŒèªžéŸ³æŽ§åˆ¶ã€‚',
            buttonNeutral: 'ç¨å¾Œè©¢å•',
            buttonNegative: 'å–æ¶ˆ',
            buttonPositive: 'å…è¨±',
          }
        );
        
        const isAuthorized = granted === PermissionsAndroid.RESULTS.GRANTED;
        
        setState(prev => ({
          ...prev,
          isAuthorized,
          authorizationStatus: isAuthorized ? 'authorized' : 'denied',
          recognitionAvailable: true,
          error: isAuthorized ? undefined : 'éº¥å…‹é¢¨æ¬Šé™è¢«æ‹’çµ•'
        }));
        
        console.log('âœ… Android microphone permission:', { granted, isAuthorized });
        return isAuthorized;
      } catch (error) {
        console.error('Error requesting Android permissions:', error);
        return await requestWebSpeechAuthorization();
      }
    } else {
      // Web å¹³å°ä½¿ç”¨ Web Speech API
      return await requestWebSpeechAuthorization();
    }
  }, []);
  
  const requestWebSpeechAuthorization = useCallback(async () => {
    try {
      // æª¢æŸ¥ Web Speech API æ”¯æ´
      const SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;
      
      if (!SpeechRecognition) {
        setState(prev => ({
          ...prev,
          error: 'Speech Recognition not supported in this browser. Please use Chrome, Edge, or Safari.',
          recognitionAvailable: false,
          authorizationStatus: 'restricted'
        }));
        return false;
      }
      
      // è«‹æ±‚éº¥å…‹é¢¨æ¬Šé™
      await navigator.mediaDevices.getUserMedia({ audio: true });
      console.log('Web microphone permission granted');
      
      setState(prev => ({
        ...prev,
        isAuthorized: true,
        authorizationStatus: 'authorized',
        recognitionAvailable: true,
        error: undefined
      }));
      
      return true;
    } catch (error) {
      console.error('Web speech authorization failed:', error);
      setState(prev => ({
        ...prev,
        error: 'Microphone permission is required for voice commands',
        isAuthorized: false,
        authorizationStatus: 'denied',
        recognitionAvailable: false
      }));
      return false;
    }
  }, []);
  
  // éšŽæ®µ 2ï¼šèªžéŸ³è¾¨è­˜åŸºç¤Žæµç¨‹
  const getSpeechLocale = useCallback((lang: string): string => {
    const map: Record<string, string> = {
      'en': 'en-US',
      'zh-TW': 'zh-TW',
      'zh-CN': 'zh-CN',
      'es': 'es-ES',
      'pt': 'pt-PT',
      'pt-BR': 'pt-BR',
      'de': 'de-DE',
      'fr': 'fr-FR',
      'ru': 'ru-RU',
      'ar': 'ar-SA',
      'ja': 'ja-JP',
      'ko': 'ko-KR',
    };
    return map[lang] ?? 'en-US';
  }, []);

  const initializeWebSpeechRecognizer = useCallback(async () => {
    try {
      if (Platform.OS !== 'web' || typeof window === 'undefined') {
        console.log('Web Speech Recognition init skipped: not on web');
        setState(prev => ({ ...prev, recognitionAvailable: false }));
        return false;
      }
      const SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;
      if (!SpeechRecognition) {
        console.log('Web Speech API not supported in this browser');
        setState(prev => ({
          ...prev,
          error: 'Speech Recognition not supported in this browser. Please use Chrome, Edge, or Safari.',
          recognitionAvailable: false,
        }));
        return false;
      }

      const recognition = new SpeechRecognition();
      recognition.continuous = false;
      recognition.interimResults = true;
      recognition.lang = getSpeechLocale(language);
      recognition.maxAlternatives = 1;

      speechRecognizerRef.current = recognition;

      console.log('âœ… Web Speech Recognition initialized with lang:', recognition.lang);
      setState(prev => ({ ...prev, recognitionAvailable: true, error: undefined }));
      return true;
    } catch (error) {
      console.error('Failed to initialize Web Speech Recognition:', error);
      setState(prev => ({
        ...prev,
        error: 'Failed to initialize speech recognition',
        recognitionAvailable: false,
      }));
      return false;
    }
  }, [language, getSpeechLocale]);

  const initializeSpeechRecognizer = useCallback(async () => {
    console.log('ðŸŽ¤ Initializing speech recognizer...');
    
    if (Platform.OS === 'ios') {
      try {
        const { SFSpeechRecognizer, AVAudioEngine } = require('react-native');
        
        const locale = getSpeechLocale(language);
        const recognizer = new SFSpeechRecognizer(locale);
        speechRecognizerRef.current = recognizer;
        
        const audioEngine = new AVAudioEngine();
        audioEngineRef.current = audioEngine;
        
        console.log('âœ… iOS Speech Framework initialized with locale:', locale);
        return true;
      } catch (error) {
        console.error('Failed to initialize iOS Speech Framework:', error);
        return await initializeWebSpeechRecognizer();
      }
    } else {
      return await initializeWebSpeechRecognizer();
    }
  }, [language, getSpeechLocale, initializeWebSpeechRecognizer]);
  
  // éšŽæ®µ 3ï¼šå…±é€šèªžéŸ³æµç¨‹ - å®Œæ•´çš„èªžéŸ³æŽ§åˆ¶æµç¨‹
  const executeVoiceControlFlow = useCallback(async (videoControls: any) => {
    console.log('ðŸŽ¤ === é–‹å§‹èªžéŸ³æŽ§åˆ¶æµç¨‹ ===');
    
    try {
      // æ­¥é©Ÿ 1ï¼šæª¢æŸ¥éº¥å…‹é¢¨èˆ‡èªžéŸ³è¾¨è­˜æ¬Šé™
      console.log('æ­¥é©Ÿ 1ï¼šæª¢æŸ¥æ¬Šé™ç‹€æ…‹');
      if (!state.isAuthorized) {
        console.log('æ¬Šé™æœªæŽˆæ¬Šï¼Œå°Žå‘æŽˆæ¬Šæµç¨‹');
        const authorized = await requestSpeechAuthorization();
        if (!authorized) {
          console.log('âŒ æ¬Šé™æŽˆæ¬Šå¤±æ•—');
          setState(prev => ({
            ...prev,
            error: 'èªžéŸ³è¾¨è­˜æ¬Šé™è¢«æ‹’çµ•ï¼Œè«‹åœ¨è¨­å®šä¸­å…è¨±éº¥å…‹é¢¨æ¬Šé™'
          }));
          return { success: false, message: 'æ¬Šé™è¢«æ‹’çµ•' };
        }
      }
      
      // æ­¥é©Ÿ 2ï¼šèª¿ç”¨æœ¬åœ°èªžéŸ³è¾¨è­˜ API
      console.log('æ­¥é©Ÿ 2ï¼šå•Ÿå‹•èªžéŸ³è¾¨è­˜');
      const recognitionSuccess = await startSpeechRecognition();
      if (!recognitionSuccess) {
        console.log('âŒ èªžéŸ³è¾¨è­˜å•Ÿå‹•å¤±æ•—');
        setState(prev => ({
          ...prev,
          error: 'èªžéŸ³è¾¨è­˜å•Ÿå‹•å¤±æ•—ï¼Œè«‹é‡è©¦'
        }));
        return { success: false, message: 'è¾¨è­˜å•Ÿå‹•å¤±æ•—' };
      }
      
      console.log('âœ… èªžéŸ³æŽ§åˆ¶æµç¨‹å•Ÿå‹•æˆåŠŸ');
      return { success: true, message: 'èªžéŸ³è¾¨è­˜å·²å•Ÿå‹•' };
    } catch (error) {
      console.error('âŒ èªžéŸ³æŽ§åˆ¶æµç¨‹éŒ¯èª¤:', error);
      setState(prev => ({
        ...prev,
        error: 'èªžéŸ³æŽ§åˆ¶æµç¨‹ç™¼ç”ŸéŒ¯èª¤'
      }));
      return { success: false, message: 'æµç¨‹åŸ·è¡ŒéŒ¯èª¤' };
    }
  }, [state.isAuthorized, requestSpeechAuthorization]);
  
  // éšŽæ®µ 3ï¼šèªžéŸ³æŒ‡ä»¤è§£æžèˆ‡å½±ç‰‡æŽ§åˆ¶ - å•Ÿå‹•èªžéŸ³è¾¨è­˜
  const startSpeechRecognition = useCallback(async () => {
    console.log('ðŸŽ¤ Starting speech recognition...');
    
    if (!state.isAuthorized) {
      const authorized = await requestSpeechAuthorization();
      if (!authorized) {
        return false;
      }
    }
    
    if (Platform.OS === 'ios' && speechRecognizerRef.current && audioEngineRef.current) {
      try {
        const { SFSpeechAudioBufferRecognitionRequest } = require('react-native');
        
        // å»ºç«‹ SFSpeechAudioBufferRecognitionRequest
        const request = new SFSpeechAudioBufferRecognitionRequest();
        recognitionRequestRef.current = request;
        
        // è¨­å®šéŸ³è¨Šå¼•æ“Ž
        const audioEngine = audioEngineRef.current;
        const inputNode = audioEngine.inputNode;
        const recordingFormat = inputNode.outputFormatForBus(0);
        
        // å®‰è£éŸ³è¨Š tap
        inputNode.installTapOnBus(0, 1024, recordingFormat, (buffer: any, when: any) => {
          request.appendAudioPCMBuffer(buffer);
        });
        
        // å•Ÿå‹•éŸ³è¨Šå¼•æ“Ž
        audioEngine.prepare();
        await audioEngine.start();
        
        // å•Ÿå‹• recognitionTask
        const recognizer = speechRecognizerRef.current;
        const task = recognizer.recognitionTaskWithRequest(request, (result: any, error: any) => {
          if (error) {
            console.error('Speech recognition error:', error);
            setState(prev => ({
              ...prev,
              error: `èªžéŸ³è¾¨è­˜éŒ¯èª¤: ${error.localizedDescription}`,
              isListening: false,
              isRecording: false
            }));
            return;
          }
          
          if (result) {
            const text = result.bestTranscription.formattedString;
            console.log('Speech recognition result:', text);
            
            setState(prev => ({
              ...prev,
              lastCommand: text.toLowerCase().trim(),
              error: undefined
            }));
            
            if (result.isFinal) {
              console.log('Final speech result:', text);
              stopSpeechRecognition();
            }
          }
        });
        
        recognitionTaskRef.current = task;
        
        setState(prev => ({
          ...prev,
          isListening: true,
          isRecording: true,
          error: undefined
        }));
        
        console.log('âœ… iOS Speech Framework recognition started');
        return true;
      } catch (error) {
        console.error('Failed to start iOS speech recognition:', error);
        return await startWebSpeechRecognition();
      }
    } else {
      return await startWebSpeechRecognition();
    }
  }, [state.isAuthorized, requestSpeechAuthorization]);
  
  const startWebSpeechRecognition = useCallback(async () => {
    try {
      if (!speechRecognizerRef.current) {
        await initializeWebSpeechRecognizer();
      }
      
      const recognition = speechRecognizerRef.current;
      if (!recognition) {
        console.log('Speech recognizer not initialized');
        setState(prev => ({
          ...prev,
          error: 'Speech Recognition not supported in this browser. Please use Chrome, Edge, or Safari.',
          isListening: false,
          isRecording: false,
        }));
        return false;
      }
      
      // Set up event handlers before starting
      recognition.onstart = () => {
        console.log('Web speech recognition started successfully');
        // reset retry counter on fresh start
        webRetryCountRef.current = 0;
        setState(prev => ({
          ...prev,
          isListening: true,
          isRecording: true,
          error: undefined
        }));
      };
      
      recognition.onresult = (event: any) => {
        try {
          if (!event.results || event.results.length === 0) {
            console.log('No speech results received');
            return;
          }
          
          const lastResult = event.results[event.results.length - 1];
          if (!lastResult || !lastResult[0]) {
            console.log('Invalid speech result structure');
            return;
          }
          
          const alt = lastResult[0];
          const text: string = alt.transcript ?? '';
          const confidence: number = typeof alt.confidence === 'number' ? alt.confidence : 0;
          if (!text || text.trim().length === 0) {
            console.log('Empty speech result received');
            return;
          }
          
          console.log('Web speech result:', text, 'isFinal:', lastResult.isFinal, 'confidence:', confidence);
          
          // Update live confidence for UI
          setState(prev => ({
            ...prev,
            lastConfidence: confidence,
            error: undefined,
          }));
          
          if (lastResult.isFinal) {
            const clean = text.toLowerCase().trim();
            // Threshold gating
            if (confidence < (state.confidenceThreshold ?? 0.7)) {
              console.log(`Confidence ${confidence.toFixed(2)} below threshold ${(state.confidenceThreshold ?? 0.7).toFixed(2)} - asking user to retry`);
              setState(prev => ({
                ...prev,
                userHint: `è¾¨è­˜ä¿¡å¿ƒä¸è¶³ (${(confidence * 100).toFixed(0)}%)ï¼Œè«‹å†è©¦ä¸€æ¬¡æˆ–èªªæ˜Žæ›´æ¸…æ¥š`,
                lastCommand: undefined,
                isProcessing: false,
              }));
            } else {
              setState(prev => ({
                ...prev,
                lastCommand: clean,
                userHint: undefined,
              }));
            }
          }
        } catch (error) {
          console.error('Error processing speech result:', error);
        }
      };
      
      recognition.onerror = (event: any) => {
        const err: string = event?.error ?? 'unknown';
        console.log('Web speech recognition error:', err);

        // Handle no-speech error gracefully with limited auto-retry
        if (err === 'no-speech') {
          console.log('No speech detected');
          setState(prev => ({
            ...prev,
            error: undefined
          }));
          if (!state.isPersistentMode && state.isRecording) {
            if (webRetryCountRef.current < webMaxRetriesRef.current && !webRestartingRef.current) {
              webRetryCountRef.current += 1;
              webRestartingRef.current = true;
              console.log(`Retrying recognition (${webRetryCountRef.current}/${webMaxRetriesRef.current})...`);
              try {
                recognition.stop();
              } catch {}
              setTimeout(() => {
                webRestartingRef.current = false;
                void startWebSpeechRecognition();
              }, 600);
            } else {
              console.log('Max no-speech retries reached or restart in progress');
            }
          }
          return;
        }
        
        // Handle aborted error (usually from manual stop or restart)
        if (err === 'aborted') {
          console.log('Recognition aborted - likely manual stop or restart');
          return;
        }

        // Handle network errors
        if (err === 'network') {
          console.log('Network error occurred during recognition');
          setState(prev => ({
            ...prev,
            error: 'Network error. Please check your internet connection.',
            isListening: false,
            isRecording: false,
          }));
          return;
        }

        // Handle microphone access errors
        if (err === 'audio-capture') {
          setState(prev => ({
            ...prev,
            error: 'Microphone access denied or not available',
            isListening: false,
            isRecording: false,
          }));
          return;
        }

        // Handle permission denied errors
        if (err === 'not-allowed') {
          setState(prev => ({
            ...prev,
            error: 'Microphone permission denied. Please allow microphone access.',
            isListening: false,
            isRecording: false,
          }));
          return;
        }

        // Handle service not available
        if (err === 'service-not-allowed') {
          setState(prev => ({
            ...prev,
            error: 'Speech recognition service not available',
            isListening: false,
            isRecording: false,
          }));
          return;
        }

        // Handle language not supported
        if (err === 'language-not-supported') {
          setState(prev => ({
            ...prev,
            error: 'Selected language not supported for speech recognition',
            isListening: false,
            isRecording: false,
          }));
          return;
        }

        // Handle other errors
        console.error('Unhandled speech recognition error:', err);
        setState(prev => ({
          ...prev,
          error: `Speech recognition error: ${err}`,
          isListening: false,
          isRecording: false,
        }));
      };
      
      recognition.onend = () => {
        console.log('Web speech recognition ended');
        // If we ended without user stop and in recording mode (single-shot), and we haven't exceeded retries, restart once more
        if (!state.isPersistentMode && state.isRecording && webRetryCountRef.current < webMaxRetriesRef.current && !webRestartingRef.current) {
          webRetryCountRef.current += 1;
          console.log(`Auto-restarting after end (${webRetryCountRef.current}/${webMaxRetriesRef.current})...`);
          setTimeout(() => {
            void startWebSpeechRecognition();
          }, 500);
          return;
        }
        setState(prev => ({
          ...prev,
          isListening: false,
          isRecording: false,
          isProcessing: false
        }));
      };
      
      // Start recognition with error handling
      try {
        recognition.start();
        console.log('âœ… Web Speech Recognition start command issued');
        return true;
      } catch (startError) {
        console.error('Error starting recognition:', startError);
        throw startError;
      }
    } catch (error) {
      console.error('Failed to start web speech recognition:', error);
      setState(prev => ({
        ...prev,
        error: 'Unable to start speech recognition. Please try again.',
        isListening: false,
        isRecording: false
      }));
      return false;
    }
  }, [initializeWebSpeechRecognizer]);
  
  const stopSpeechRecognition = useCallback(() => {
    console.log('ðŸ›‘ Stopping speech recognition...');
    
    if (Platform.OS === 'ios') {
      try {
        // åœæ­¢ iOS Speech Framework
        if (recognitionTaskRef.current) {
          recognitionTaskRef.current.cancel();
          recognitionTaskRef.current = null;
        }
        
        if (recognitionRequestRef.current) {
          recognitionRequestRef.current.endAudio();
          recognitionRequestRef.current = null;
        }
        
        if (audioEngineRef.current) {
          audioEngineRef.current.stop();
          audioEngineRef.current.inputNode.removeTapOnBus(0);
        }
        
        console.log('âœ… iOS Speech Framework stopped');
      } catch (error) {
        console.error('Error stopping iOS speech recognition:', error);
      }
    }
    
    // åœæ­¢ Web Speech Recognition
    if (speechRecognizerRef.current && speechRecognizerRef.current.stop) {
      try {
        speechRecognizerRef.current.stop();
        console.log('âœ… Web Speech Recognition stopped');
      } catch (error) {
        console.error('Error stopping web speech recognition:', error);
      }
    }
    
    setState(prev => ({
      ...prev,
      isListening: false,
      isRecording: false,
      isProcessing: false
    }));
  }, []);
  
  // å•Ÿå‹•æŒçºŒç›£è½æ¨¡å¼
  const startPersistentListening = useCallback(async () => {
    try {
      console.log('Starting persistent listening mode...');
      
      if (Platform.OS === 'web') {
        const SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;
        
        if (SpeechRecognition) {
          // Stop any existing recognition first
          if (persistentRecognitionRef.current) {
            console.log('Stopping existing persistent recognition');
            persistentRecognitionRef.current.stop();
            persistentRecognitionRef.current = null;
          }
          
          // Clear any existing restart timeout
          if (restartTimeoutRef.current) {
            clearTimeout(restartTimeoutRef.current);
            restartTimeoutRef.current = null;
          }
          
          const recognition = new SpeechRecognition();
          recognition.continuous = true;
          recognition.interimResults = false;
          recognition.lang = getSpeechLocale(language);
          recognition.maxAlternatives = 1;
          
          // Add timeout settings to prevent hanging
          if ('grammars' in recognition) {
            // Some browsers support additional settings
            try {
              (recognition as any).serviceURI = undefined; // Use default service
            } catch {
              // Ignore if not supported
            }
          }

          recognition.onstart = () => {
            console.log('Persistent voice recognition started successfully');
            setState(prev => ({ 
              ...prev, 
              isListening: true,
              isPersistentMode: true,
              error: undefined 
            }));

            // Start max session timer (<= 1 minute per spec)
            if (maxSessionTimerRef.current) {
              clearTimeout(maxSessionTimerRef.current);
            }
            maxSessionTimerRef.current = setTimeout(() => {
              console.log('Max session reached (55s). Restarting recognition for continuous listening...');
              try {
                recognition.stop();
              } catch (e) {
                console.log('Error stopping for max session restart:', e);
              }
            }, 55000); // 55 seconds to stay well under the 60s limit
          };

          recognition.onresult = (event: any) => {
            try {
              if (!event.results || event.results.length === 0) {
                console.log('No speech results in persistent mode');
                return;
              }
              
              const lastResult = event.results[event.results.length - 1];
              if (!lastResult || !lastResult[0]) {
                console.log('Invalid speech result structure in persistent mode');
                return;
              }
              
              if (lastResult.isFinal) {
                const text = lastResult[0].transcript;
                const confidence: number = typeof lastResult[0].confidence === 'number' ? lastResult[0].confidence : 0;
                if (!text || text.trim().length === 0) {
                  console.log('Empty speech result in persistent mode');
                  return;
                }
                
                const cleanText = text.toLowerCase().trim();
                console.log('Persistent voice command received:', cleanText, 'confidence:', confidence);
                if (confidence < (state.confidenceThreshold ?? 0.7)) {
                  setState(prev => ({ 
                    ...prev, 
                    userHint: `è¾¨è­˜ä¿¡å¿ƒä¸è¶³ (${(confidence * 100).toFixed(0)}%)ï¼Œè«‹å†è©¦ä¸€æ¬¡`,
                    lastConfidence: confidence,
                  }));
                } else {
                  setState(prev => ({ 
                    ...prev, 
                    lastCommand: cleanText,
                    lastConfidence: confidence,
                    error: undefined 
                  }));
                }
              }
            } catch (error) {
              console.error('Error processing speech result in persistent mode:', error);
            }
          };

          recognition.onerror = (event: any) => {
            console.log('Speech recognition event:', event.error);
            
            // Handle different types of errors in persistent mode
            if (event.error === 'no-speech') {
              console.log('No speech detected in persistent mode - this is normal, will restart automatically');
              // For no-speech in persistent mode, this is expected behavior
              // The recognition will restart via onend handler
              // Don't show error to user as this is normal
              setState(prev => ({ 
                ...prev, 
                error: undefined // Clear any previous errors
              }));
              return;
            } else if (event.error === 'audio-capture') {
              console.error('Audio capture error - microphone may not be available');
              setState(prev => ({ 
                ...prev, 
                error: 'Microphone access denied or not available',
                isListening: false,
                isPersistentMode: false
              }));
              return;
            } else if (event.error === 'not-allowed') {
              console.error('Microphone permission denied');
              setState(prev => ({ 
                ...prev, 
                error: 'Microphone permission denied. Please allow microphone access.',
                isListening: false,
                isPersistentMode: false
              }));
              return;
            } else if (event.error === 'network') {
              console.log('Network error - will retry...');
              // Network errors are recoverable, continue listening
            } else if (event.error === 'aborted') {
              console.log('Recognition aborted - likely due to restart');
              // Aborted is normal when we restart recognition
              return;
            } else {
              console.error('Other speech recognition error:', event.error);
              setState(prev => ({ 
                ...prev, 
                error: `Speech recognition error: ${event.error}`,
                isListening: false 
              }));
            }
            
            // For recoverable errors (network, etc.), try to restart after a delay
            if (event.error === 'network') {
              if (restartTimeoutRef.current) {
                clearTimeout(restartTimeoutRef.current);
              }
              restartTimeoutRef.current = setTimeout(() => {
                setState(prev => {
                  if (prev.isPersistentMode) {
                    console.log('Restarting after network error...');
                    startPersistentListening();
                  }
                  return prev;
                });
              }, 3000); // Longer delay for network recovery
            }
          };

          recognition.onend = () => {
            console.log('Recognition session ended naturally');
            
            // Clear any existing restart timeout
            if (restartTimeoutRef.current) {
              clearTimeout(restartTimeoutRef.current);
            }
            // Clear max session timer
            if (maxSessionTimerRef.current) {
              clearTimeout(maxSessionTimerRef.current);
              maxSessionTimerRef.current = null;
            }
            
            // Auto-restart if still in persistent mode and not manually stopped
            setState(prev => {
              if (prev.isPersistentMode && persistentRecognitionRef.current) {
                console.log('Auto-restarting recognition after natural end...');
                // Restart after a short delay for normal end events
                restartTimeoutRef.current = setTimeout(() => {
                  // Check if we're still in persistent mode before restarting
                  setState(currentState => {
                    if (currentState.isPersistentMode) {
                      startPersistentListening();
                    }
                    return currentState;
                  });
                }, 1000); // Increased delay to prevent rapid restarts
              } else {
                console.log('Not restarting - persistent mode disabled or manually stopped');
              }
              return prev;
            });
          };

          console.log('Starting speech recognition...');
          recognition.start();
          persistentRecognitionRef.current = recognition;
        } else {
          console.error('Speech Recognition not supported in this browser');
          setState(prev => ({ 
            ...prev, 
            error: 'Speech Recognition not supported in this browser. Please use Chrome, Edge, or Safari.',
            isPersistentMode: false
          }));
        }
      } else {
        // For mobile, we'll implement a different approach
        console.log('Mobile persistent listening - using alternative approach');
        setState(prev => ({ 
          ...prev, 
          isPersistentMode: true,
          isListening: true,
          error: undefined,
          userHint: 'æŒçºŒç›£è½ä¸­ï¼Œéš¨æ™‚å¯èªªå‡ºæŒ‡ä»¤'
        }));
        
        // TODO: Implement mobile-specific persistent listening
        // This could use expo-speech or other mobile-specific APIs
      }
    } catch (error) {
      console.error('Failed to start persistent listening:', error);
      setState(prev => ({ 
        ...prev, 
        error: 'Failed to start persistent listening mode',
        isPersistentMode: false,
        isListening: false
      }));
    }
  }, [language]);

  const stopPersistentListening = useCallback(() => {
    console.log('Stopping persistent listening mode...');
    
    // Clear any restart timeouts
    if (restartTimeoutRef.current) {
      clearTimeout(restartTimeoutRef.current);
      restartTimeoutRef.current = null;
    }
    // Clear max session timer
    if (maxSessionTimerRef.current) {
      clearTimeout(maxSessionTimerRef.current);
      maxSessionTimerRef.current = null;
    }
    
    // Stop the recognition
    if (persistentRecognitionRef.current) {
      try {
        persistentRecognitionRef.current.stop();
        persistentRecognitionRef.current = null;
        console.log('Persistent recognition stopped successfully');
      } catch (error) {
        console.error('Error stopping persistent recognition:', error);
      }
    }
    
    // Update state
    setState(prev => ({ 
      ...prev, 
      isListening: false,
      isPersistentMode: false,
      error: undefined
    }));
    
    console.log('Persistent listening stopped completely');
  }, []);

  const togglePersistentMode = useCallback(async () => {
    console.log('Toggling persistent mode. Current state:', state.isPersistentMode);
    
    if (state.isPersistentMode) {
      stopPersistentListening();
    } else {
      // æª¢æŸ¥æŽˆæ¬Šç‹€æ…‹
      if (!state.isAuthorized) {
        const authorized = await requestSpeechAuthorization();
        if (!authorized) {
          return;
        }
      }
      
      await startPersistentListening();
    }
  }, [state.isPersistentMode, state.isAuthorized, startPersistentListening, stopPersistentListening, requestSpeechAuthorization]);

  const startRecording = useCallback(async () => {
    try {
      console.log('ðŸŽ¤ Starting single voice recording...');
      
      // æª¢æŸ¥æŽˆæ¬Šç‹€æ…‹
      if (!state.isAuthorized) {
        const authorized = await requestSpeechAuthorization();
        if (!authorized) {
          return;
        }
      }
      
      // ä½¿ç”¨ Speech Framework é€²è¡Œå–®æ¬¡èªžéŸ³è¾¨è­˜
      const success = await startSpeechRecognition();
      if (!success) {
        console.error('Failed to start speech recognition');
        return;
      }
      
      if (Platform.OS !== 'web') {
        await Haptics.impactAsync(Haptics.ImpactFeedbackStyle.Light);
      }
    } catch (error) {
      console.error('Failed to start recording:', error);
      setState(prev => ({ ...prev, error: t('voice.error') }));
    }
  }, [t, state.isAuthorized, requestSpeechAuthorization, startSpeechRecognition]);

  const stopRecording = useCallback(async (): Promise<string | null> => {
    try {
      console.log('ðŸ›‘ Stopping voice recording...');
      
      setState(prev => ({ ...prev, isProcessing: true }));
      
      // åœæ­¢èªžéŸ³è¾¨è­˜
      stopSpeechRecognition();
      
      if (Platform.OS !== 'web') {
        await Haptics.notificationAsync(Haptics.NotificationFeedbackType.Success);
      }
      
      // çµæžœæœƒé€éŽ callback å›žå‚³ï¼Œé€™è£¡è¿”å›ž null
      return null;
    } catch (error) {
      console.error('Failed to stop recording:', error);
      setState(prev => ({ 
        ...prev, 
        isProcessing: false, 
        error: t('voice.error') 
      }));
      
      if (Platform.OS !== 'web') {
        await Haptics.notificationAsync(Haptics.NotificationFeedbackType.Error);
      }
      
      return null;
    }
  }, [t, stopSpeechRecognition]);

  const toggleRecording = useCallback(async () => {
    if (state.isRecording) {
      return await stopRecording();
    } else {
      await startRecording();
      return null;
    }
  }, [state.isRecording, stopRecording, startRecording]);

  // éšŽæ®µ 1ï¼šåˆå§‹åŒ–èªžéŸ³è¾¨è­˜ç’°å¢ƒ
  useEffect(() => {
    const initializeVoiceRecognition = async () => {
      console.log('ðŸŽ¤ Initializing voice recognition system...');
      
      // åˆå§‹åŒ–èªžéŸ³è¾¨è­˜å™¨
      await initializeSpeechRecognizer();
      
      // æª¢æŸ¥æŽˆæ¬Šç‹€æ…‹ï¼ˆä½†ä¸è‡ªå‹•è«‹æ±‚ï¼‰
      if (Platform.OS === 'ios') {
        try {
          const { SFSpeechRecognizer } = require('react-native');
          const authStatus = SFSpeechRecognizer.authorizationStatus();
          
          setState(prev => ({
            ...prev,
            authorizationStatus: authStatus,
            isAuthorized: authStatus === 'authorized',
            recognitionAvailable: SFSpeechRecognizer.isSupported()
          }));
        } catch (error) {
          console.log('iOS Speech Framework not available, using web fallback');
        }
      }
    };
    
    // å»¶é²åˆå§‹åŒ–ä»¥é¿å… hydration å•é¡Œ
    const timer = setTimeout(initializeVoiceRecognition, 1000);
    return () => clearTimeout(timer);
  }, [initializeSpeechRecognizer]);

  // Listen for voice command results and show feedback
  useEffect(() => {
    if (state.lastCommand) {
      console.log('New voice command received:', state.lastCommand);
      // Clear the command after a delay to reset the UI
      const timer = setTimeout(() => {
        setState(prev => ({ ...prev, lastCommand: undefined }));
      }, 3000);
      return () => clearTimeout(timer);
    }
  }, [state.lastCommand]);

  // éšŽæ®µ 4ï¼šæŒ‡ä»¤è§£æžå™¨ - åŒ¹é…å…§å»ºæŒ‡ä»¤æˆ–è‡ªå®šç¾©æŒ‡ä»¤
  const parseVoiceCommand = useCallback((text: string) => {
    if (!text) return null;
    
    const command = text.toLowerCase().trim();
    console.log('ðŸ” è§£æžèªžéŸ³æŒ‡ä»¤:', command);
    
    // æª¢æŸ¥è‡ªå®šç¾©æŒ‡ä»¤
    const customCommand = Object.entries(customCommands).find(([_, trigger]) => 
      command.includes(trigger.toLowerCase())
    );
    
    if (customCommand) {
      const [commandId] = customCommand;
      console.log('âœ… åŒ¹é…åˆ°è‡ªå®šç¾©æŒ‡ä»¤:', commandId);
      return { type: 'custom', commandId, originalText: text };
    }
    
    // å…§å»ºæŒ‡ä»¤åŒ¹é…
    const builtInCommands = {
      // æ’­æ”¾æŽ§åˆ¶
      play: ['play', 'æ’­æ”¾', 'å†ç”Ÿ', 'reproducir', 'jouer', 'spielen', 'Ð²Ð¾ÑÐ¿Ñ€Ð¾Ð¸Ð·Ð²ÐµÑÑ‚Ð¸', 'ØªØ´ØºÙŠÙ„', 'ìž¬ìƒ', 'start', 'å¼€å§‹æ’­æ”¾', 'æ’­æ”¾è§†é¢‘'],
      pause: ['pause', 'æš«åœ', 'ä¸€æ™‚åœæ­¢', 'pausar', 'pause', 'pausieren', 'Ð¿Ð°ÑƒÐ·Ð°', 'Ø¥ÙŠÙ‚Ø§Ù Ù…Ø¤Ù‚Øª', 'ì¼ì‹œì •ì§€', 'æš‚åœ'],
      stop: ['stop', 'åœæ­¢', 'parar', 'arrÃªter', 'stoppen', 'Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ', 'ØªÙˆÙ‚Ù', 'ì •ì§€', 'åœæ­¢æ’­æ”¾'],
      
      // å¿«è½‰å€’è½‰
      forward10: ['forward 10', 'skip 10', 'forward ten', 'å¿«è½‰10ç§’', 'å¿«è¿›10ç§’', '10ç§’é€²ã‚€', 'adelantar 10', 'avancer 10', 'Ð²Ð¿ÐµÑ€ÐµÐ´ 10', 'ØªÙ‚Ø¯ÙŠÙ… 10', '10ì´ˆ ì•žìœ¼ë¡œ', 'å‘å‰åç§’'],
      forward20: ['forward 20', 'skip 20', 'forward twenty', 'å¿«è½‰20ç§’', 'å¿«è¿›20ç§’', '20ç§’é€²ã‚€', 'adelantar 20', 'avancer 20', 'Ð²Ð¿ÐµÑ€ÐµÐ´ 20', 'ØªÙ‚Ø¯ÙŠÙ… 20', '20ì´ˆ ì•žìœ¼ë¡œ', 'å‘å‰äºŒåç§’'],
      forward30: ['forward 30', 'skip 30', 'forward thirty', 'å¿«è½‰30ç§’', 'å¿«è¿›30ç§’', '30ç§’é€²ã‚€', 'adelantar 30', 'avancer 30', 'Ð²Ð¿ÐµÑ€ÐµÐ´ 30', 'ØªÙ‚Ø¯ÙŠÙ… 30', '30ì´ˆ ì•žìœ¼ë¡œ', 'å‘å‰ä¸‰åç§’'],
      backward10: ['backward 10', 'back 10', 'rewind 10', 'å€’è½‰10ç§’', 'åŽé€€10ç§’', 'å¿«é€€10ç§’', '10ç§’æˆ»ã‚‹', 'retroceder 10', 'reculer 10', 'Ð½Ð°Ð·Ð°Ð´ 10', 'ØªØ±Ø§Ø¬Ø¹ 10', '10ì´ˆ ë’¤ë¡œ', 'å‘åŽåç§’'],
      backward20: ['backward 20', 'back 20', 'rewind 20', 'å€’è½‰20ç§’', 'åŽé€€20ç§’', 'å¿«é€€20ç§’', '20ç§’æˆ»ã‚‹', 'retroceder 20', 'reculer 20', 'Ð½Ð°Ð·Ð°Ð´ 20', 'ØªØ±Ø§Ø¬Ø¹ 20', '20ì´ˆ ë’¤ë¡œ', 'å‘åŽäºŒåç§’'],
      backward30: ['backward 30', 'back 30', 'rewind 30', 'å€’è½‰30ç§’', 'åŽé€€30ç§’', 'å¿«é€€30ç§’', '30ç§’æˆ»ã‚‹', 'retroceder 30', 'reculer 30', 'Ð½Ð°Ð·Ð°Ð´ 30', 'ØªØ±Ø§Ø¬Ø¹ 30', '30ì´ˆ ë’¤ë¡œ', 'å‘åŽä¸‰åç§’'],
      
      // éŸ³é‡æŽ§åˆ¶
      volumeUp: ['volume up', 'louder', 'increase volume', 'éŸ³é‡èª¿é«˜', 'éŸ³é‡è°ƒé«˜', 'éŸ³é‡ä¸Šã’ã‚‹', 'subir volumen', 'augmenter volume', 'Ð³Ñ€Ð¾Ð¼Ñ‡Ðµ', 'Ø±ÙØ¹ Ø§Ù„ØµÙˆØª', 'ë³¼ë¥¨ ì˜¬ë¦¬ê¸°', 'å¤§å£°ä¸€ç‚¹'],
      volumeDown: ['volume down', 'quieter', 'decrease volume', 'éŸ³é‡èª¿ä½Ž', 'éŸ³é‡è°ƒä½Ž', 'éŸ³é‡ä¸‹ã’ã‚‹', 'bajar volumen', 'baisser volume', 'Ñ‚Ð¸ÑˆÐµ', 'Ø®ÙØ¶ Ø§Ù„ØµÙˆØª', 'ë³¼ë¥¨ ë‚´ë¦¬ê¸°', 'å°å£°ä¸€ç‚¹'],
      volumeMax: ['max volume', 'volume max', 'maximum volume', 'éŸ³é‡æœ€å¤§', 'æœ€å¤§éŸ³é‡', 'volumen mÃ¡ximo', 'volume maximum', 'Ð¼Ð°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ Ð³Ñ€Ð¾Ð¼ÐºÐ¾ÑÑ‚ÑŒ', 'Ø£Ù‚ØµÙ‰ ØµÙˆØª', 'ìµœëŒ€ ë³¼ë¥¨', 'éŸ³é‡è°ƒåˆ°æœ€å¤§'],
      mute: ['mute', 'éœéŸ³', 'é™éŸ³', 'ãƒŸãƒ¥ãƒ¼ãƒˆ', 'silenciar', 'muet', 'stumm', 'Ð±ÐµÐ· Ð·Ð²ÑƒÐºÐ°', 'ÙƒØªÙ… Ø§Ù„ØµÙˆØª', 'ìŒì†Œê±°'],
      unmute: ['unmute', 'è§£é™¤éœéŸ³', 'å–æ¶ˆé™éŸ³', 'ãƒŸãƒ¥ãƒ¼ãƒˆè§£é™¤', 'activar sonido', 'activer son', 'Ð·Ð²ÑƒÐº Ð²ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ', 'Ø¥Ù„ØºØ§Ø¡ ÙƒØªÙ… Ø§Ù„ØµÙˆØª', 'ìŒì†Œê±° í•´ì œ'],
      
      // æ’­æ”¾é€Ÿåº¦
      speed05: ['0.5 speed', 'half speed', 'slow', '0.5å€é€Ÿ', '0.5 é€Ÿåº¦', '0.5 velocidad', '0.5 vitesse', '0.5 ÑÐºÐ¾Ñ€Ð¾ÑÑ‚ÑŒ', 'Ø³Ø±Ø¹Ø© 0.5', '0.5ë°°ì†', 'åŠé€Ÿ'],
      speed1: ['normal speed', '1x speed', '1 speed', 'regular speed', 'æ­£å¸¸é€Ÿåº¦', 'æ­£å¸¸', 'é€šå¸¸é€Ÿåº¦', 'velocidad normal', 'vitesse normale', 'Ð¾Ð±Ñ‹Ñ‡Ð½Ð°Ñ ÑÐºÐ¾Ñ€Ð¾ÑÑ‚ÑŒ', 'Ø§Ù„Ø³Ø±Ø¹Ø© Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©', 'ì •ìƒ ì†ë„'],
      speed125: ['1.25 speed', '1.25x speed', '1.25å€é€Ÿ', '1.25 é€Ÿåº¦', '1.25 velocidad', '1.25 vitesse', '1.25 ÑÐºÐ¾Ñ€Ð¾ÑÑ‚ÑŒ', 'Ø³Ø±Ø¹Ø© 1.25', '1.25ë°°ì†'],
      speed15: ['1.5 speed', '1.5x speed', 'fast', '1.5å€é€Ÿ', '1.5 é€Ÿåº¦', '1.5 velocidad', '1.5 vitesse', '1.5 ÑÐºÐ¾Ñ€Ð¾ÑÑ‚ÑŒ', 'Ø³Ø±Ø¹Ø© 1.5', '1.5ë°°ì†', 'åŠ å¿«'],
      speed2: ['2 speed', '2x speed', 'double speed', '2å€é€Ÿ', '2 é€Ÿåº¦', '2 velocidad', '2 vitesse', '2 ÑÐºÐ¾Ñ€Ð¾ÑÑ‚ÑŒ', 'Ø³Ø±Ø¹Ø© 2', '2ë°°ì†', 'ä¸¤å€é€Ÿ'],
      
      // å…¨èž¢å¹•
      fullscreen: ['fullscreen', 'full screen', 'enter fullscreen', 'å…¨èž¢å¹•', 'å…¨å±', 'ãƒ•ãƒ«ã‚¹ã‚¯ãƒªãƒ¼ãƒ³', 'pantalla completa', 'plein Ã©cran', 'Ð¿Ð¾Ð»Ð½Ñ‹Ð¹ ÑÐºÑ€Ð°Ð½', 'Ù…Ù„Ø¡ Ø§Ù„Ø´Ø§Ø´Ø©', 'ì „ì²´í™”ë©´'],
      exitFullscreen: ['exit fullscreen', 'leave fullscreen', 'é›¢é–‹å…¨èž¢å¹•', 'é€€å‡ºå…¨å±', 'ãƒ•ãƒ«ã‚¹ã‚¯ãƒªãƒ¼ãƒ³çµ‚äº†', 'salir pantalla completa', 'quitter plein Ã©cran', 'Ð²Ñ‹Ð¹Ñ‚Ð¸ Ð¸Ð· Ð¿Ð¾Ð»Ð½Ð¾Ð³Ð¾ ÑÐºÑ€Ð°Ð½Ð°', 'Ø§Ù„Ø®Ø±ÙˆØ¬ Ù…Ù† Ù…Ù„Ø¡ Ø§Ù„Ø´Ø§Ø´Ø©', 'ì „ì²´í™”ë©´ ë‚˜ê°€ê¸°'],
      
      // å…¶ä»–åŠŸèƒ½
      bookmark: ['bookmark', 'add bookmark', 'mark', 'æ›¸ç±¤', 'ä¹¦ç­¾', 'ãƒ–ãƒƒã‚¯ãƒžãƒ¼ã‚¯', 'marcador', 'marque-page', 'Ð·Ð°ÐºÐ»Ð°Ð´ÐºÐ°', 'Ø¥Ø´Ø§Ø±Ø© Ù…Ø±Ø¬Ø¹ÙŠØ©', 'ë¶ë§ˆí¬'],
      favorite: ['favorite', 'add favorite', 'like', 'æœ€æ„›', 'æ”¶è—', 'ãŠæ°—ã«å…¥ã‚Š', 'favorito', 'favori', 'Ð¸Ð·Ð±Ñ€Ð°Ð½Ð½Ð¾Ðµ', 'Ù…ÙØ¶Ù„', 'ì¦ê²¨ì°¾ê¸°'],
    };
    
    // å°‹æ‰¾åŒ¹é…çš„å…§å»ºæŒ‡ä»¤
    for (const [action, triggers] of Object.entries(builtInCommands)) {
      if (triggers.some(trigger => command.includes(trigger.toLowerCase()))) {
        console.log('âœ… åŒ¹é…åˆ°å…§å»ºæŒ‡ä»¤:', action);
        return { type: 'builtin', commandId: action, originalText: text };
      }
    }
    
    console.log('âŒ æœªæ‰¾åˆ°åŒ¹é…çš„æŒ‡ä»¤:', command);
    return null;
  }, [customCommands]);
  
  // éšŽæ®µ 5ï¼šå½±ç‰‡æŽ§åˆ¶å°æ‡‰å‹•ä½œ
  const executeCommand = useCallback(async (commandId: string, videoControls: any) => {
    try {
      console.log('ðŸŽ¬ åŸ·è¡Œå½±ç‰‡æŽ§åˆ¶æŒ‡ä»¤:', commandId);
      
      if (!videoControls.uri) {
        console.log('âŒ æœªè¼‰å…¥å½±ç‰‡ï¼Œç„¡æ³•åŸ·è¡ŒæŒ‡ä»¤');
        return { success: false, message: 'è«‹å…ˆè¼‰å…¥å½±ç‰‡' };
      }
      
      if (!videoControls.player) {
        console.log('âŒ å½±ç‰‡æ’­æ”¾å™¨ä¸å¯ç”¨ï¼Œç„¡æ³•åŸ·è¡ŒæŒ‡ä»¤');
        return { success: false, message: 'æ’­æ”¾å™¨ä¸å¯ç”¨' };
      }
      
      switch (commandId) {
        case 'play':
          await videoControls.play();
          break;
        case 'pause':
          await videoControls.pause();
          break;
        case 'stop':
          await videoControls.stop();
          break;
        case 'forward10':
          await videoControls.seek(10);
          break;
        case 'forward20':
          await videoControls.seek(20);
          break;
        case 'forward30':
          await videoControls.seek(30);
          break;
        case 'backward10':
          await videoControls.seek(-10);
          break;
        case 'backward20':
          await videoControls.seek(-20);
          break;
        case 'backward30':
          await videoControls.seek(-30);
          break;
        case 'volumeUp':
          await videoControls.setVolume(Math.min(1, videoControls.volume + 0.1));
          break;
        case 'volumeDown':
          await videoControls.setVolume(Math.max(0, videoControls.volume - 0.1));
          break;
        case 'volumeMax':
          await videoControls.setVolume(1);
          break;
        case 'mute':
          await videoControls.setVolume(0);
          break;
        case 'unmute':
          const currentVolume = videoControls.volume || 0;
          await videoControls.setVolume(currentVolume > 0 ? currentVolume : 1);
          break;
        case 'speed05':
          await videoControls.setSpeed(0.5);
          break;
        case 'speed1':
        case 'speed10':
          await videoControls.setSpeed(1);
          break;
        case 'speed125':
          await videoControls.setSpeed(1.25);
          break;
        case 'speed15':
          await videoControls.setSpeed(1.5);
          break;
        case 'speed2':
        case 'speed20':
          await videoControls.setSpeed(2);
          break;
        case 'fullscreen':
        case 'exitFullscreen':
          videoControls.toggleFullscreen();
          break;
        case 'bookmark':
          await videoControls.addBookmark();
          break;
        case 'favorite':
          await videoControls.toggleFavorite();
          break;
        default:
          console.log('âŒ æœªçŸ¥æŒ‡ä»¤:', commandId);
          return { success: false, message: `æœªçŸ¥æŒ‡ä»¤: ${commandId}` };
      }
      
      console.log('âœ… æŒ‡ä»¤åŸ·è¡ŒæˆåŠŸ:', commandId);
      return { success: true, message: `å·²åŸ·è¡Œ: ${commandId}` };
    } catch (error) {
      console.error('âŒ æŒ‡ä»¤åŸ·è¡ŒéŒ¯èª¤:', error);
      return { success: false, message: `åŸ·è¡ŒéŒ¯èª¤: ${error instanceof Error ? error.message : 'æœªçŸ¥éŒ¯èª¤'}` };
    }
  }, []);

  // éšŽæ®µ 6ï¼šå®Œæ•´èªžéŸ³æŒ‡ä»¤è™•ç†æµç¨‹
  const processVoiceCommand = useCallback(async (text: string, videoControls: any) => {
    console.log('ðŸŽ¯ === é–‹å§‹è™•ç†èªžéŸ³æŒ‡ä»¤ ===');
    console.log('è¼¸å…¥æ–‡å­—:', text);
    
    if (!text) {
      console.log('âŒ ç©ºç™½æŒ‡ä»¤');
      return { success: false, message: 'æœªæŽ¥æ”¶åˆ°èªžéŸ³æŒ‡ä»¤' };
    }
    
    try {
      // æ­¥é©Ÿ 4ï¼šå‘¼å«æŒ‡ä»¤è§£æžå™¨
      console.log('æ­¥é©Ÿ 4ï¼šè§£æžèªžéŸ³æŒ‡ä»¤');
      const parsedCommand = parseVoiceCommand(text);
      
      if (!parsedCommand) {
        console.log('âŒ æŒ‡ä»¤è§£æžå¤±æ•— - æœªæ‰¾åˆ°åŒ¹é…çš„æŒ‡ä»¤');
        return { success: false, message: `ç„¡æ³•è­˜åˆ¥æŒ‡ä»¤: "${text}"` };
      }
      
      console.log('âœ… æŒ‡ä»¤è§£æžæˆåŠŸ:', parsedCommand);
      
      // æ­¥é©Ÿ 5ï¼šè§¸ç™¼å½±ç‰‡æŽ§åˆ¶å°æ‡‰å‹•ä½œ
      console.log('æ­¥é©Ÿ 5ï¼šåŸ·è¡Œå½±ç‰‡æŽ§åˆ¶å‹•ä½œ');
      const executionResult = await executeCommand(parsedCommand.commandId, videoControls);
      
      // æ­¥é©Ÿ 6ï¼šå›žé¥‹åŸ·è¡Œçµæžœçµ¦ä½¿ç”¨è€… UI
      console.log('æ­¥é©Ÿ 6ï¼šå›žé¥‹åŸ·è¡Œçµæžœ');
      if (executionResult.success) {
        console.log('âœ… èªžéŸ³æŒ‡ä»¤åŸ·è¡ŒæˆåŠŸ:', executionResult.message);
        setState(prev => ({
          ...prev,
          error: undefined,
          lastCommand: text
        }));
      } else {
        console.log('âŒ èªžéŸ³æŒ‡ä»¤åŸ·è¡Œå¤±æ•—:', executionResult.message);
        setState(prev => ({
          ...prev,
          error: executionResult.message
        }));
      }
      
      console.log('ðŸŽ¯ === èªžéŸ³æŒ‡ä»¤è™•ç†å®Œæˆ ===');
      return executionResult;
    } catch (error) {
      console.error('âŒ èªžéŸ³æŒ‡ä»¤è™•ç†ç™¼ç”ŸéŒ¯èª¤:', error);
      const errorMessage = error instanceof Error ? error.message : 'æœªçŸ¥éŒ¯èª¤';
      setState(prev => ({
        ...prev,
        error: `æŒ‡ä»¤è™•ç†éŒ¯èª¤: ${errorMessage}`
      }));
      return { success: false, message: `è™•ç†éŒ¯èª¤: ${errorMessage}` };
    }
  }, [parseVoiceCommand, executeCommand]);



  // Cleanup on unmount
  useEffect(() => {
    return () => {
      stopPersistentListening();
      if (recognitionRef.current) {
        recognitionRef.current.stop();
      }
      if (mediaRecorderRef.current) {
        mediaRecorderRef.current.stop();
      }
      if (maxSessionTimerRef.current) {
        clearTimeout(maxSessionTimerRef.current);
      }
    };
  }, [stopPersistentListening]);

  return useMemo(() => ({
    ...state,
    customCommands,
    saveCustomCommand,
    setConfidenceThreshold,
    startRecording,
    stopRecording,
    toggleRecording,
    processVoiceCommand,
    executeCommand,
    parseVoiceCommand,
    executeVoiceControlFlow,
    startPersistentListening,
    stopPersistentListening,
    togglePersistentMode,
    requestSpeechAuthorization,
    startSpeechRecognition,
    stopSpeechRecognition,
  }), [state, customCommands, saveCustomCommand, setConfidenceThreshold, startRecording, stopRecording, toggleRecording, processVoiceCommand, executeCommand, parseVoiceCommand, executeVoiceControlFlow, startPersistentListening, stopPersistentListening, togglePersistentMode, requestSpeechAuthorization, startSpeechRecognition, stopSpeechRecognition]);
});